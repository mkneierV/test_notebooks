{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install apache-beam[gcp]\n",
    "pip install apitools\n",
    "pip install six==1.10\n",
    "# Note: May need to restart kernel after install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\n",
      "apache-beam (2.16.0)\n",
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\n",
      "tensorflow (1.8.0)\n",
      "tensorflow-estimator (1.14.0)\n",
      "tensorflow-hub (0.6.0)\n",
      "tensorflow-metadata (0.14.0)\n",
      "tensorflow-probability (0.7.0)\n",
      "tensorflow-serving-api (1.14.0)\n",
      "tensorflow-transform (0.14.0)\n",
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\n",
      "apitools (0.1.4)\n",
      "google-apitools (0.5.28)\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep \"beam\"\n",
    "!pip list | grep \"tensorflow\"\n",
    "!pip list | grep \"apitools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: For using tf transform, ensure versions are compatible with: https://pypi.org/project/tensorflow-transform/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"weight_pounds\", \"is_male\", \"mother_age\", \"mother_race\", \"plurality\", \"gestation_weeks\"]\n",
    "\n",
    "def get_source_query(step):\n",
    "    train_years = (1980,2004)\n",
    "    eval_years  = (2005,2007)\n",
    "    test_years  = (2008, 2008)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      weight_pounds,\n",
    "      is_male,\n",
    "      mother_age,\n",
    "      mother_race,\n",
    "      plurality,\n",
    "      gestation_weeks\n",
    "    FROM\n",
    "      publicdata.samples.natality\n",
    "    WHERE year BETWEEN {} AND {}\n",
    "      AND weight_pounds > 0\n",
    "      AND mother_age > 0\n",
    "      AND plurality > 0\n",
    "      AND gestation_weeks > 0\n",
    "      AND month > 0\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    if step == 'eval':\n",
    "        source_query = query.format(*eval_years)\n",
    "    elif step == 'test':\n",
    "        source_query = query.format(*test_years)\n",
    "    elif step == \"train\":\n",
    "        source_query = query.format(*train_years)\n",
    "    else:\n",
    "        raise ValueError(\"step value of {} must be one of 'train', 'eval', 'test'\".format(step))\n",
    "    return source_query\n",
    "\n",
    "\n",
    "def read_from_bq(pipeline, step):\n",
    "    source_query = get_source_query(step)\n",
    "    raw_data = (\n",
    "        pipeline\n",
    "        | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(\n",
    "            beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "        | '{} - Clean up Data'.format(step) >> beam.Map(prep_bq_row)\n",
    "    )\n",
    "    \n",
    "    raw_metadata = create_raw_metadata()\n",
    "    raw_dataset = (raw_data, raw_metadata)\n",
    "    return raw_dataset\n",
    "\n",
    "\n",
    "def prep_bq_row(bq_row):\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = {} \n",
    "    \n",
    "    for feature_name in bq_row.keys():\n",
    "        result[feature_name] = str(bq_row[feature_name])\n",
    "\n",
    "    if 'mother_race' in bq_row and bq_row['mother_race'] in races:\n",
    "        result['mother_race'] = races[bq_row['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def to_csv_string(bq_dict):\n",
    "    FEATURES = [\"weight_pounds\", \"is_male\", \"mother_age\", \"mother_race\", \"plurality\", \"gestation_weeks\"]\n",
    "    output = []\n",
    "    for f in FEATURES:\n",
    "        output.append(bq_dict[f])\n",
    "        \n",
    "    return \",\".join(output)\n",
    "\n",
    "\n",
    "def write_csv(transformed_data, location, step): \n",
    "    FEATURES = [\"weight_pounds\", \"is_male\", \"mother_age\", \"mother_race\", \"plurality\", \"gestation_weeks\"]\n",
    "    (\n",
    "        transformed_data \n",
    "        | '{} - Write Transformed Data'.format(step) >> beam.io.WriteToText(\n",
    "            file_path_prefix=os.path.join(location, step, step),\n",
    "            file_name_suffix=\".csv\",\n",
    "            header=\",\".join(FEATURES)\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformation_pipeline(args):\n",
    "    \n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "    runner = args['runner']\n",
    "    transformed_data_location = args['transformed_data_location']\n",
    "    transform_artifact_location = args['transform_artifact_location']\n",
    "    temporary_dir = args['temporary_dir']\n",
    "    \n",
    "    print(\"Sink transformed data files location: {}\".format(transformed_data_location))\n",
    "    print(\"Sink transform artifact location: {}\".format(transform_artifact_location))\n",
    "    print(\"Temporary directory: {}\".format(temporary_dir))\n",
    "    print(\"Runner: {}\".format(runner))\n",
    "\n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:            \n",
    "        for step in (\"train\", \"eval\", \"test\"):\n",
    "            source_query = get_source_query(step)\n",
    "            print('query built')\n",
    "            data = (\n",
    "                pipeline\n",
    "                | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(\n",
    "                    beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "                | '{} - Clean up Data'.format(step) >> beam.Map(prep_bq_row)\n",
    "                | '{} - Prepare for csv'.format(step) >> beam.Map(to_csv_string)\n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.WriteToText(\n",
    "                    file_path_prefix=os.path.join(transformed_data_location, step, step),\n",
    "                    file_name_suffix=\".csv\")\n",
    "            )\n",
    "            # Write transformed train data to sink in csv\n",
    "            # write_csv(data, transformed_data_location, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "RUN_LOCAL = False\n",
    "PROJECT = 'dogwood-actor-255317'\n",
    "BUCKET = 'mkneier-gcs-cloudml'\n",
    "REGION = 'us-west1'\n",
    "ROOT_DIR = 'bwt_data'\n",
    "\n",
    "OUTPUT_DIR = ROOT_DIR if RUN_LOCAL else \"gs://{}/{}\".format(BUCKET, ROOT_DIR)\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "\n",
    "runner = 'DirectRunner' if RUN_LOCAL == True else 'DataflowRunner'\n",
    "\n",
    "job_name = 'preprocess-babweight-data-tft-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'transformed_data_location':  TRANSFORMED_DATA_DIR,\n",
    "    'transform_artifact_location':  TRANSFORM_ARTIFACTS_DIR,\n",
    "    'temporary_dir': TEMP_DIR,\n",
    "    'debug':False,\n",
    "    \n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'worker_machine_type': 'n1-standard-1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_transformation_pipeline(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
